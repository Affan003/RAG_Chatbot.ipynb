{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing required modules\n",
        "\n"
      ],
      "metadata": {
        "id": "CAp8C5UR5ScT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28\n",
        "!pip install cohere\n",
        "!pip install tiktoken\n",
        "!pip install -qU openai pinecone-client datasets\n"
      ],
      "metadata": {
        "id": "kvZqjArmw9I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# get API key from top-right dropdown on OpenAI website\n",
        "openai.api_key = \"enter_openai_api_key\"\n",
        "\n"
      ],
      "metadata": {
        "id": "mxEIgMYCxDl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who was the 12th person on the moon and when did they land?\""
      ],
      "metadata": {
        "id": "UsQUGGQCEunH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def complete(prompt):\n",
        "\n",
        "    res = openai.Completion.create(\n",
        "        engine='gpt-3.5-turbo-instruct',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=400,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        stop=None\n",
        "    )\n",
        "    return res['choices'][0]['text'].strip()\n"
      ],
      "metadata": {
        "id": "NvEvhEh-xM5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = (\n",
        "    \"Which training method should I use for sentence transformers when \" +\n",
        "    \"I only have pairs of related sentences?\"\n",
        ")\n",
        "\n",
        "complete(query)"
      ],
      "metadata": {
        "id": "3RHJdui0xWPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating A knowledge base"
      ],
      "metadata": {
        "id": "t4RpiyVH5gum"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9x4vNx66RNa"
      },
      "outputs": [],
      "source": [
        "embed_model = \"text-embedding-ada-002\"\n",
        "\n",
        "res = openai.Embedding.create(\n",
        "    input=[\n",
        "        \"Sample document text goes here\",\n",
        "        \"there will be several phrases in each batch\"\n",
        "    ], engine=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res.keys()"
      ],
      "metadata": {
        "id": "LGZ2S-5k0RZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(res['data'])"
      ],
      "metadata": {
        "id": "xHf6mZw30ULb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(res['data'][0]['embedding']), len(res['data'][1]['embedding'])"
      ],
      "metadata": {
        "id": "CrM5nWt-0cnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Data"
      ],
      "metadata": {
        "id": "y4XNiDYq5ypF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset('jamescalam/youtube-transcriptions', split='train')\n",
        "data"
      ],
      "metadata": {
        "id": "Le16na7x0jv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "id": "eSyGneUB0rZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "new_data = []\n",
        "\n",
        "window = 20  # number of sentences to combine\n",
        "stride = 4  # number of sentences to 'stride' over, used to create overlap\n",
        "\n",
        "for i in tqdm(range(0, len(data), stride)):\n",
        "    i_end = min(len(data)-1, i+window)\n",
        "    if data[i]['title'] != data[i_end]['title']:\n",
        "        # in this case we skip this entry as we have start/end of two videos\n",
        "        continue\n",
        "    text = ' '.join(data[i:i_end]['text'])\n",
        "    # create the new merged dataset\n",
        "    new_data.append({\n",
        "        'start': data[i]['start'],\n",
        "        'end': data[i_end]['end'],\n",
        "        'title': data[i]['title'],\n",
        "        'text': text,\n",
        "        'id': data[i]['id'],\n",
        "        'url': data[i]['url'],\n",
        "        'published': data[i]['published'],\n",
        "        'channel_id': data[i]['channel_id']\n",
        "    })"
      ],
      "metadata": {
        "id": "XpzR1rkX0xcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data[0]"
      ],
      "metadata": {
        "id": "UI5698Ju0-6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "\n",
        "index_name = 'openai-youtube-transcriptions'\n",
        "\n",
        "# Initialize connection to Pinecone (get API key at app.pinecone.io)\n",
        "pc = pinecone.Pinecone(api_key=\"Enter_openai_api_key\", environment=\"us-east1-gcp\")\n",
        "\n",
        "# Check if index already exists (it shouldn't if this is the first time)\n",
        "if index_name not in pc.list_indexes():\n",
        "    # If it does not exist, create the index\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=len(res['data'][0]['embedding']),\n",
        "        metric='cosine',\n",
        "        spec=pinecone.ServerlessSpec(cloud='aws', region='us-west-2')\n",
        "    )\n",
        "\n",
        "# Connect to the index\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# View index stats\n",
        "index.describe_index_stats()\n"
      ],
      "metadata": {
        "id": "WNe7o7-a1GyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from time import sleep\n",
        "\n",
        "batch_size = 100  # how many embeddings we create and insert at once\n",
        "\n",
        "for i in tqdm(range(0, len(new_data), batch_size)):\n",
        "    # find end of batch\n",
        "    i_end = min(len(new_data), i+batch_size)\n",
        "    meta_batch = new_data[i:i_end]\n",
        "    # get ids\n",
        "    ids_batch = [x['id'] for x in meta_batch]\n",
        "    # get texts to encode\n",
        "    texts = [x['text'] for x in meta_batch]\n",
        "    # create embeddings (try-except added to avoid RateLimitError)\n",
        "    done = False\n",
        "    while not done:\n",
        "        try:\n",
        "            res = openai.Embedding.create(input=texts, engine=embed_model)\n",
        "            done = True\n",
        "        except:\n",
        "            sleep(5)\n",
        "    embeds = [record['embedding'] for record in res['data']]\n",
        "    # cleanup metadata\n",
        "    meta_batch = [{\n",
        "        'start': x['start'],\n",
        "        'end': x['end'],\n",
        "        'title': x['title'],\n",
        "        'text': x['text'],\n",
        "        'url': x['url'],\n",
        "        'published': x['published'],\n",
        "        'channel_id': x['channel_id']\n",
        "    } for x in meta_batch]\n",
        "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
        "    # upsert to Pinecone\n",
        "    index.upsert(vectors=to_upsert)"
      ],
      "metadata": {
        "id": "AS4_4kRx4bv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = openai.Embedding.create(\n",
        "    input=[query],\n",
        "    engine=embed_model\n",
        ")\n",
        "\n",
        "# retrieve from Pinecone\n",
        "xq = res['data'][0]['embedding']\n",
        "\n",
        "# get relevant contexts (including the questions)\n",
        "res = index.query(xq, top_k=2, include_metadata=True)"
      ],
      "metadata": {
        "id": "UnrJLXx54e-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "id": "AjS9JnAr4lio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "limit = 3750\n",
        "\n",
        "def retrieve(query):\n",
        "    res = openai.Embedding.create(\n",
        "        input=[query],\n",
        "        engine=embed_model\n",
        "    )\n",
        "\n",
        "    # retrieve from Pinecone\n",
        "    xq = res['data'][0]['embedding']\n",
        "\n",
        "    # get relevant contexts\n",
        "    res = index.query(xq, top_k=3, include_metadata=True)\n",
        "    contexts = [\n",
        "        x['metadata']['text'] for x in res['matches']\n",
        "    ]\n",
        "\n",
        "    # build our prompt with the retrieved contexts included\n",
        "    prompt_start = (\n",
        "        \"Answer the question based on the context below.\\n\\n\"+\n",
        "        \"Context:\\n\"\n",
        "    )\n",
        "    prompt_end = (\n",
        "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    )\n",
        "    # append contexts until hitting limit\n",
        "    for i in range(1, len(contexts)):\n",
        "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
        "            prompt = (\n",
        "                prompt_start +\n",
        "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
        "                prompt_end\n",
        "            )\n",
        "            break\n",
        "        elif i == len(contexts)-1:\n",
        "            prompt = (\n",
        "                prompt_start +\n",
        "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
        "                prompt_end\n",
        "            )\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "tmTHAfhj4ySg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first we retrieve relevant items from Pinecone\n",
        "query_with_contexts = retrieve(query)\n",
        "query_with_contexts"
      ],
      "metadata": {
        "id": "pQyoWqKs4vio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then we complete the context-infused query\n",
        "complete(query_with_contexts)"
      ],
      "metadata": {
        "id": "ySHWUVc044V_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}